## NLP (up to LLM) TimeLine: [[{]]
- NLP studies the interactions between computers and human language
- As old as computers themselves.

1949 (1st models) Grammar Theories Warren Weaver's memoradum
                  1st to suggest algorithmic approach to Machine Translation (MT)
                  leading to ...
1954 (1st models) Georgetown experiment
                  1st computer application to MT
1957 (Grammar Theories) Chomsky established the first grammar theory (Generative Grammar)
1964 (1st models) ELIZA (& 1968 SHRDLU)
                  They can be considered to be the first NL understanding computer programs.

1965 (Grammar Theories) Transformational generative grammars
1968 (Grammar Theories) Case Ggrammar
1969 (Grammar Theories) Semantic networks
1972 (Grammar Theories) Conceptual Dependeny Theory

 70s the concept of "conceptual ontologies" became quite fashionable.
     similar to knowledge graphs: concepts are linked by their associations.
     Ontology: formal naming and definition of the types, properties and
               interrelationships of the entities that exist in a particular
               domain of discourse.

1975 (Statistical Model) MARGIE, (1976) TaleSpin,(1977) QUALM,(1978) SAM,
                         (1978) PAM, (1979) Politics,(1981) Plot Units.


1983 (Symbolic NLP) Charniak proposes "Passing Markers", a mechanism for resolving
     ambiguities in language comprehension by indicating the
     relationship between adjacent words.

1986 (Symbolic NLP) Riesbeck&Martin proposed "Uniform Parsing" a new approach
     to NLP combining parsing&inferencing in a uniform framework.

1987 (Symbolic NLP) Semantic Interpretation
     proposed by Hirst: new approach to resolve ambiguity:

1989 (Statistical Models) Tree based model to predict the next word
                          in a sentence, by Balh.
1990 (Statistical Models) Linguistic Structure from text statistics
1990 (Statistical Models) Statistical Parsing of Message
1990 (Statistical Models) XX alignment models
1991,(Statistical Models) Brown proposes method for aligning sentences
      ·                ·  in parallel corpora for machine translation apps.
      └────────────────┴─ It was the beginning of thinking about language
                          as a probabilistic process, showing th potential of
                          statistical parsing techniques for processing messages

2003 (Neural NLP) First NLP Model by Bengio, a simple feed-forward model.
2008 (Neural NLP) multi-task learning with ConvNet by Collobert&Weston.
2011 (Neural NLP) generative text model with RNN by Hinton.
2013 (Neural NLP) Word2Vec, introduced by Mikolov.
2014 (Neural NLP) Sequence to sequence learning, suggested by Sutskever.
2015 (Neural NLP) Attestion Mechanism.
2017 (Neural NLP) Transformers architecture, by Vaswani
                  led to a revolution in model performance.
2018 (Neural NLP) BERT, presented by Devlin and popularizing Transformers.
2022 (Neural NLP) ChatGPT, NLP for the public.
[[}]]

## Universal Transformer: [[{]]
@[https://arxiv.org/pdf/1807.03819.pdf] "White-paper"
@[https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html]
- Transformer. new machine learning model showing remarkable success over
  existing algorithms for machine translation (MT) and other language
  understanding tasks.
- Previously most neural network based approaches to machine translation
  relied on recurrent neural networks (RNNs) which operate sequentially
  (e.g. translating words in a sentence one-after-the-other) using
  recurrence (i.e. the output of each step feeds into the next).
    While RNNs are very powerful at modeling sequences, their sequential
  nature means that they are quite slow to train, as longer sentences
  need more processing steps, and their recurrent structure also makes
  them notoriously difficult to train properly.

- In contrast, THE TRANSFORMER USED NO RECURRENCE, INSTEAD PROCESSING ALL
  WORDS OR SYMBOLS IN THE SEQUENCE IN PARALLEL WHILE MAKING USE OF A
  SELF-ATTENTION MECHANISM TO INCORPORATE CONTEXT FROM WORDS FARTHER AWAY.
- By processing all words in parallel and letting each word attend to other
  words in the sentence over multiple processing steps, THE TRANSFORMER
  WAS MUCH FASTER TO TRAIN THAN RECURRENT MODELS.
  REMARKABLY, IT ALSO YIELDED MUCH BETTER TRANSLATION RESULTS.
- WARN: on smaller and more structured language understanding tasks,
        or even simple algorithmic tasks such as copying a string
        the Transformer does NOT perform very well.
        IN CONTRAST: models that perform well on these tasks, like the
        Neural GPU and Neural Turing Machine, fail on large-scale
        language understanding tasks like translation.

- C&P from @[https://www.infoq.com/news/2020/09/google-bigbird-nlp/]
"...BigBird allows Transformer NN to process sequences up to 8x
   longer achieving state-of-the-art performance levels on
   natural-language processing (NLP) AND GENOMICS TASKS!!! ..."

- the "self-attention" mechanism allows the network to "remember"
  previous items in the sequence can be executed in parallel
  on the entire sequence, which speeds up training and inference.
- However, since self-attention can link (or "attend") each item
  in the sequence to every other item, the COMPUTATIONAL AND
  MEMORY COMPLEXITY of self-attention is O(n^2), where n is the
  maximum sequence length that can be processed IMPOSSING A
  PRACTICAL LIMIT ON SEQUENCE LENGTH, AROUND 512 ITEMS, THAT
  CAN BE HANDLED BY CURRENT HARDWARE.

[[}]]

## NLP vs NLU vs NL: [[{cognitive.nlp]]
- NLP (Natural Language Processing)
  broad term describing technics to "ingest what is said", break it down,
  comprehend its meaning, determine appropriate action, and respond back
  in a language the user will understand.
- NLU (Natural Language Understanding)
  much narrower part of NLP dealing with how to best handle unstructured inputs
  and convert them into a structured form that a machine can understand
  and act upon: handling mispronunciations, contractions, colloquialisms,...
- NLG (Natural Language Generation).
  "what happens when computers write language"
  NLG processes turn structured data into text.

  E.g.: A Chatbot is a full -middleware- application making use of NLP/NLU/NLG
        as well as other resources like front-ends, backend databases, ...)
[[}]]


## Non-Classified [[{01_PM.backlog]]

## NLP Java Tools: [[{cognitive.nlp]]
- OpenNLP  : text tokenization, part-of-speech tagging, chunking, etc. (tutorial)
- Stanford : probabilistic natural language parsers, both highly optimized PCFG*
  Parser     and lexicalized dependency parsers, and a lexicalized PCFG parser
- TPT      : Standford (T)opic (M)odelling (T)oolbox: CVB0 algorithm, etc.
- ScalaNLP : Natural Language Processing and machine learning.
- Snowball : stemmer, (C and Java)
- MALLET   : statistical natural language processing, document classification,
             clustering, topic modeling, information extraction, and other machine
            learning applications to text.
- JGibbLDA : LDA in Java
- Lucene   : (Apache) stop-words removal and stemming

See also:
JAVA RNN How-to
- @[https://www.programcreek.com/2017/07/recurrent-neural-network-example-ai-programmer-1/]
- @[https://www.programcreek.com/2017/07/build-an-ai-programmer-using-recurrent-neural-network-2/]
- @[https://www.programcreek.com/2017/07/build-an-ai-programmer-using-recurrent-neural-network-3/]
- @[https://www.programcreek.com/2017/02/different-types-of-recurrent-neural-network-structures/]
[[}]]

## InstructGPT (OpenAI, 2022): LM to Follow Human Instructions [[{]]
@[https://www.infoq.com/news/2022/02/openai-instructgpt/]
[[}]]

## NLTK: @[https://www.nltk.org/]  [[{]]
Toolkit for building Python programs to work with human language data.
- easy-to-use interfaces to over 50 corpora and lexical resources (WordNet,...)
- suite of text processing libraries for CLASSIFICATION, TOKENIZATION,
  STEMMING, TAGGING, PARSING, SEMANTIC REASONING.
- wrappers for industrial-strength NLP libraries.

  import nltk
  sentence = """At eight o'clock on Thursday morning Arthur didn't feel very good."""
  tokens = nltk.word_tokenize(sentence) # ← ['At', 'eight', "o'clock", 'on', 'Thursday', 'morning',
                                        #    'Arthur', 'did', "n't", 'feel', 'very', 'good', '.']
  tagged = nltk.pos_tag(tokens)
  tagged[0:6]                           # ← [('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'),
                                             ('Thursday', 'NNP'), ('morning', 'NN')]
[[}]]

## Google ALBERT NLP Model: [[{]]
@[https://www.infoq.com/news/2020/01/google-albert-ai-nlp/]
· deep-learning natural language processing (NLP) model, which uses 89%
  fewer parameters than the state-of-the-art BERT model, with little
  loss of accuracy. The model can also be scaled-up to achieve new
  state-of-the-art performance on NLP benchmarks.
[[}]]

## Blender ChatBot now OOSS (by Facebook) [[{]]
@[https://www.infoq.com/news/2020/04/facebook-blender-chatbot/]
@[https://parl.ai/projects/blender/]
- open-domain chatbot.
- "... 1st chatbot that has learned to blend several
  conversation skills, including the ability to show
  empathy and discuss nearly any topics ..."
- training: up to 9.4 billion parameters and
            techniques for blending skills and detailed generation.

"... we show that other ingredients and skills that an expert
  conversationalist blends in a seamless way:
  PROVIDE ENGAGING TALKING POINTS AND LISTENING TO THEIR PARTNERS,
  BOTH ASKING AND ANSWERING QUESTIONS, AND DISPLAYING KNOWLEDGE,
  EMPATHY AND PERSONALITY APPROPRIATELY, DEPENDING ON THE
  SITUATION.
  ... We then discuss the limitations of this work by analyzing
  failure cases of our models....2
[[}]]

## Stanza (Standford NLP Toolkit): [[{]]
https://www.infoq.com/news/2020/03/stanza-nlp-toolkit/
- features:
  - language-agnostic fully neural pipeline for text analysis
    (supporting 66 human languages)
  - python interface to Stanford's CoreNLP java software.

- Can be used for:
  tokenization, multi-word token expansion, lemmatization,
  part-of-speech and morphological feature tagging, dependency parsing,
  and named-entity recognition (NER).
- Compared to existing popular NLP toolkits, Stanza aims to support more
  human languages, increase accuracy in text analysis tasks, and remove
  the need for any preprocessing by providing a unified framework for
  processing raw human language text.
[[}]]

## AWS SaaS Textract [[{cloud,use_case.text_processing,01_PM.low_code]]
@[https://press.aboutamazon.com/news-releases/news-release-details/aws-announces-general-availability-amazon-textract]
- fully managed service to automatically extract text and data,
  including tables and forms, in any document without the need
  for manual review, custom code, or machine learning experience.
[[}]]

## Snps NLU: [[{01_PM.TODO]]
https://github.com/snipsco/snips-nlu
Snips NLU is a Natural Language Understanding python library that
allows to parse sentences written in natural language, and extract
structured information.
It’s the library that powers the NLU engine used in the Snips
Console that you can use to create awesome and private-by-design
voice assistants.
[[}]]

## RoBERTa NLP [[{01_PM.TODO]]
https://www.infoq.com/news/2019/09/facebook-roberta-nlp/
[[}]]

## Researchers Open-Source Model Explanation Toolkit AllenNLP Interpret [[{cognitive.nlp,01_PM.TODO]]
https://www.infoq.com/news/2019/10/nlp-model-explanation/ [[}]]

## ExBERT Tool: Exploring NLP Models Learned Representations: [[{cognitive.nlp,01_PM.TODO]]
@[https://www.infoq.com/news/2020/04/exbert-explainable-nlp/]
[[}]]

## Tool for Universal Communication [[{cognitive.*,01_PM.TODO]]
@[https://www.gtd.es/es/blog/ingenieros-leoneses-crean-una-herramienta-informatica-para-la-comunicacion-universal]
- Se trata de un programa con una sintaxis universal que permite la
  comunicación inmediata entre personas de distintos idiomas
[[}]]

[[{cognitive.human_translation,01_PM.TODO]]
## DeepL: "Google Traductor ++?"
@[https://wwwhatsnew.com/2020/03/23/deepl-la-alternativa-a-google-traductor-ya-traduce-japones-y-chino/]
[[}]]

## Protege: [[{cognitive.nlp,data.ontology,01_PM.TODO]]
@[https://protege.stanford.edu/products.php]

- Protégé is a free, open-source platform that provides a growing
  user community with a suite of tools to construct domain models and
  knowledge-based applications with ontologies.
[[}]]

## Pattern-Exploiting Training: Exceeds GPT-3 Perf. 99.9% Fewer Params:
[[{cognitive.nlp,nlp.101,comparative,GPT-3,01_PM.TODO]]
@[https://www.infoq.com/news/2020/10/training-exceeds-gpt3/]
· Using PET, the team trained a Transformer NLP model with 223M
  parameters that out-performed the 175B-parameter GPT-3 by over 3
  percentage points on the SuperGLUE benchmark.
[[}]]

## The Pile, 825 GB NLP Data set:
[[{data.open_data,01_PM.TODO]]
@[https://pile.eleuther.ai/]
@[https://arxiv.org/abs/2101.00027]
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe,
Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima,
Shawn Presser, Connor Leahy

· Recent work has demonstrated that increased training dataset
  diversity improves general cross-domain knowledge and downstream
  generalization capability for large-scale language models. With this
  in mind, we present the Pile:
  · 825 GiB English text corpus targeted at training large-scale language models.
  · constructed from 22 diverse high-quality subsets
    -- both existing and newly constructed -- many of which derive from
    academic or professional sources.
· Our evaluation of the untuned performance of GPT-2 and GPT-3 on
  the Pile shows that these models struggle on many of its components,
  such as academic writing. Conversely, models trained on the Pile
  improve significantly over both Raw CC and CC-100 on all components
  of the Pile, while improving performance on downstream evaluations.
  Through an in-depth exploratory analysis, we document potentially
  concerning aspects of the data for prospective users. We make
  publicly available the code used in its construction.
· The format of the Pile is jsonlines data compressed using zstandard.
[[}]]

[[{cognitive.NLP,rlang,01_PM.TODO]]
## 7 Excellent R NLP Tools
@[https://www.linuxlinks.com/excellent-r-natural-language-processing-tools/]
[[}]]


## nlp: ontology101.pdf @[./ontology101.pdf]

## https://www.infoq.com/news/2019/08/Baidu-OpenSources-ERNIE/ [[{,02_doc_has.comparative]]
  Baidu Open-Sources ERNIE 2.0, Beats BERT in Natural Language Processing Tasks
  In a recent blog post, Baidu, the Chinese search engine and
  e-commerce giant, announced their latest open-source, natural
  language understanding framework called ERNIE 2.0. They also shared
  recent test results, including achieving state-of-the art (SOTA)
  results and outperforming existing frameworks, including Google’s
  BERT and XLNet in 16 NLP tasks in both Chinese and English. [[}]]

## Google Open-Sources Token-Free Language Model ByT5
  https://www.infoq.com/news/2021/07/google-byt5-nlp/
  Google Research has open-sourced ByT5, a natural language processing
  (NLP) AI model that operates on raw bytes instead of abstract tokens.
  Compared to baseline models, ByT5 is more accurate on several
  benchmark tasks and is more robust to misspellings and noise.

## Ml, nlp: Google Open-Sources Trillion-Parameter AI Language Model Switch Transformer
  https://www.infoq.com/news/2021/02/google-trillion-parameter-ai/

## https://github.com/will-thompson-k/tldr-transformers
  The "tl;dr" on a few notable papers on Transformers and modern NLP.
....

## ML Meta Open-Sources 175 Billion Parameter AI Language Model OPT
https://www.infoq.com/news/2022/06/meta-opt-175b/ 
The model was trained on a dataset containing 180B tokens and
exhibits performance comparable with GPT-3, while only requiring
1/7th GPT-3's training carbon footprint.The release was announced in
a blog post written by Meta researchers Susan Zhang, Mona Diab, and
Luke Zettlemoyer. To help promote open and reproducible research in
AI, Meta has released not only the code and trained model weights,
but also a full operational logbook documenting challenges
encountered during the training process. The model is released under
a non-commercial license and is intended for use by researchers
"affiliated with organizations in government, civil society, and
academia" as well as industry researchers. 
The Transformer deep-learning architecture has become the de-facto
standard for language models, and researchers have achieved
impressive results by increasing the size of both the models and the
training datasets. Much of the research has focused on
auto-regressive decoder-only models, such as GPT-3 and PaLM, which
can perform as well as the average human on many natural language
processing (NLP) benchmarks. Although some research organizations,

https://ai4bi.beehiiv.com/p/custom-llm-chatbot-on-your-enterprise-data
## Build a Custom LLM Chatbot on Your Enterprise Data
that keeps sensitive information secure.
How to leverage the Retrieval-Augmented Generation (RAG) Architecture for your use case
Author: Tobias Zwingmann (2023-04-28)

Chatbots can streamline processes and save time, but building one
without exposing private data is challenging.

We'll discuss the Retrieval-Augmented Generation (RAG) Architecture,
a solution that helps protect sensitive data while still delivering a
great chatbot experience.

Let's dive in!
Understanding the Retrieval-Augmented Generation Architecture (RAG)

Most enterprise customers are still hesitant to touch LLM technology
because they are afraid that:

    it's hard to do technically

    it requires exposing their private data

    it's still experimental and not ready for production

    it requires them to fine-tune an LLM which is expensive

The Retrieval-Augmented Generation Architecture addresses these
concerns.

To make things clear up front:

There isn't a single best way to build your custom chatbot.

Depending on your setup it might indeed be very hard or you might
indeed need to fine-tune an LLM over multiple rounds of iteration.

But the RAG-Architecture is usually a good place to start.

Let's look at it from a high level and a concrete use case:
Step 1: Problem Statement

Here’s our example:

Our enterprise needs an internal chatbot that can provide
straightforward answers to common HR-related questions from
employees, such as how to apply for leave and where to find specific
policy documents.

The kind of problem you want to solve dictates the architecture of
your chatbot.

Here are some clarifying questions you should anwer for yourself:

    What's the goal of your chatbot?

    Will it be used internally or externally?

    Which data sources will your chatbot access?

    Do you need to consider individual access rights?

    Who should be involved in the development process?

    Should the chatbot be able to cite sources and provide links?

In our example, the main purpose of the chatbot is to provide
HR-related guidance to internal employees to reduce the workload of
HR staff. The information is largely stored in PDF documents, and we
want to cite the source from which the bot got the answer and provide
links to relevant policy documents.
Step 2: Solution Breakdown

The RAG architecture (see figure below) solves this problem by
integrating two key components: a Retriever component a Generator
component.

The Retriever component is responsible for finding relevant
information based on the user's input, while the Generator uses this
information to provide a natural language response.

Besides that, a user-facing application will handle the interface and
integration of the two components.

Let's walk through this architecture step-by-step and explore what's
happening here.
User Layer

The user layer is the frontend that the user interacts with. When
designing an application like this, it's important to be clear about
the requirements and user stories.

That means:

What should the chatbot be able to do and how will users interact
with it?

This has nothing to do with AI.

Just good old UX design.

In our example, let's say it's an internal web application where
users can enter a text query. They will receive answers in the chat
window related to HR policies and procedures, with a hyperlink to the
original documents if necessary.

The chatbot should refuse to answer any question that is not found in
the document. For example, if a user asks it about the weather or the
CEO's performance, it will provide a defined response such as "I
can't answer that".

We will learn how to implement this later.
Data Layer

Before we dive into the Generator and Retriever components, let's
quickly discuss the data layer of this use case.

As mentioned above, we're not training or fine-tuning an LLM. So the
data layer would consist only of the documents you want the chatbot
to have access to, such as PDFs, HTML, Office documents, or any other
format that the Retriever component can work with.

To keep things simple in our example, let's assume all files are
stored in a GDrive folder and we're working mostly with PDFs.

We will also store the queries and answers in a separate log file so
that we can access it later for monitoring.

Time to look into the analysis layer!
Analysis Layer

The analysis layer is where "the magic happens".

In this layer, we need three key components:

    The Generator (the LLM)

    The Retriever (the search model)

    The App Server (our backend)

To demonstrate the interplay between them, let's look at an example.
Understanding the question entry

Suppose the user asks the following question:

"How many days can I take off?"

The language model (Generator) will interpret the user's input and
analyze it to understand their intent. It will then format this into
a query that the Retriever can work to fetch the relevant documents.

Example keywords: “Working time regulations”, “holidays”,
“vacation”

The Retriever can be literally any search component - from a
classical keyword-based search, to a vector-based semantic search
powered by another LLM or a combination of both approaches. In
enterprise cases, you could also rely on a fully managed search
service like Azure Cognitive Search or Google Cloud Search.

The strength of this architecture lies in its ability to leverage the
flexibility of the LLM to comprehend the user's intent and to provide
information to the Retriever in an appropriate way (e. g. keywords or
embeddings).

That makes it a powerful combination to understand what the user
really wants and get the most relevant documents.

The Retriever will then go and find the top N relevant documents from
our file system.

So what's next?
Generating the answer

Instead of just showing the documents in a list form, we can use the
Generator again to summarize and paraphrase the relevant information
in a conversational style that is easy for users to understand.

So instead of just showing the vacation policy document, the
Generator could formulate an answer like:

"You can take 28 days of paid leave and an additional 30 days of
unpaid leave, subject to your manager's approval. See these documents
for more information: ..."

To create this answer, the Generator takes the relevant snippets
found by the retriever and adds them directly to the prompt as
additional context - so there's no fine-tuning involved.

This way, the conversation feels more natural and engaging, and the
users get the information they need immediately, without going
through a long list of documents.

So how do we control the behavior of the chatbot and prevent it from
answering questions that are not found in the documents?

Well, we simply specify the desired behavior in the system prompt.
For example, we could create a system prompt like this:

Act as a document that I am having a conversation with. Your name is
"AI Assistant". You will provide me with answers from the given
context. If the answer is not included, say exactly "I don’t know,
please contact HR". Refuse to answer any question not about the
context.

That's how we deploy a chatbot without exposing private data and
controlled behavior.
Step 3: Limitations

But wait a minute! you might say.

The LLM can still access the user inputs and also the relevant parts
from the documents that the user searched for. So don't we expose
sensitive information here?

It’s true that there is still a risk of exposing sensitive
information when using this architecture, as the LLM handles the user
inputs and sees parts of the relevant documents.

This is where you need to balance things off.

If you have highly sensitive documents, it's essential to prevent any
private data from being leaked during the information retrieval
process. One way to do this is by deploying or building your own LLM
in-house.

On the other hand, if you are more concerned with trustworthy data
processing that occurs within your organization or a specific region,
you can try using enterprise-integrated LLMs such as OpenAI on
Microsoft Azure which lets you deploy any OpenAI model within your
Azure subscription in your desired region. With this approach, you
can have contracts and policies in place to control your data and
ensure that it's not exposed or used for further training.

To sum up, deploying a chatbot that preserves customer privacy
requires thoughtful consideration of the data sources and resources
used.
Step 4: Next steps

Of course, we've only covered the tip of the iceberg. If you wanted
to bring a use case like this into production there are some more
things you should consider.

Examples include:

    Access management

    Monitoring the performance

    Controlling the LLM behavior and guardrails

While none of these points is trivial, they are also far away from
being insurmountable challenges. The RAG architecture provides you
with a solid foundation to build upon, and with proper planning and
execution, a privacy-preserving chatbot can be successfully deployed
for your organization.

Feel free to reach out if you need help!
Conclusion

I hope you have seen how you can deploy a custom LLM-based chatbot
while keeping data secure using the RAG architecture.

If you like, you can dive deeper from there, focusing on either the
information retrieval or generation parts, and consider different
options for building the chatbot.

As always, thanks for reading.

Hit reply and let me know what think - I'd love to hear from you!


    Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
           https://arxiv.org/pdf/2005.11401.pdf?utm_source=earizon.github.com/txt_world_domination

[[}]]

## Mr. Ranedeer: Tutor Prompt for personalized learning experiences! [[{prompt_engineer,use_case.learning]]
* Unlock the potential of GPT-4 with Mr. Ranedeer AI Tutor, a
  customizable prompt that delivers personalized learning experiences
  for users with diverse needs and interests.
https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor

Mr. Ranedeer AI Tutor allows you to:

    Adjust the depth of knowledge to match your learning needs
    Customize your learning style, communication type, tone, and reasoning framework
    Create the ultimate AI tutor tailored just for you

{
  "ai_tutor": {
    "Author": "JushBJJ",
    "name": "Mr. Ranedeer",
    "version": "2.0",
    "features": {
      "personalization": {
        "depth": {
          "description": "This is the depth of the content the student wants to learn. A low depth will cover the basics, and generalizations while a high depth will cover the specifics, details, unfamiliar, complex, and side cases. The lowest depth level is 1, and the highest is 10.",
          "depth_levels": {
            "Level_1": "Surface level: ....",
            "Level_2": "Expanded understanding: ....",
            "Level_3": "Detailed analysis: Provides in-depth explanations....",
            "Level_4": "Practical application: Focuses on real-world applications...",
            "Level_5": "Advanced concepts: Introduces advanced techniques and tools...",
            "Level_6": "Critical evaluation: Encourages critical thinking, questioning assumptions, and analyzing arguments to form independent opinions.",
            "Level_7": "Synthesis and integration: Synthesizes knowledge from various sources, connecting topics and themes for comprehensive understanding.",
            "Level_8": "Expert insight: Provides expert insight into nuances, complexities, and challenges, discussing trends, debates, and controversies.",
            "Level_9": "Specialization: Focuses on specific subfields, delving into specialized knowledge and fostering expertise in chosen areas.",
            "Level_10": "Cutting-edge research: Discusses recent research and discoveries, offering deep understanding of current developments and future directions."
          }
        },
        "learning_styles": {
          "Sensing": "Concrete, practical, oriented towards facts and procedures.",
          "Visual *REQUIRES PLUGINS*": "Prefer visual representations of presented material ",
          "Inductive": "Prefer presentations that proceed from the specific to the general",
          "Active": "Learn by trying things out, experimenting, and doing",
          "Sequential": "Linear, orderly learn in small incremental steps",
          "Intuitive": "Conceptual, innovative, oriented toward theories and meanings",
          "Verbal": "Prefer written and spoken explanations",
          "Deductive": "Prefer presentations that go from the general to the specific",
          "Reflective": "Learn by thinking things through, working alone",
          "Global": "Holistic, system thinkers, learn in large leaps"
        },
        "communication_styles": {
          "stochastic": "Incorporates randomness or variability, generating slight variations in responses for a dynamic, less repetitive conversation.",
          "Formal": "Follows strict grammatical rules and avoids contractions, slang, or colloquialisms for a structured and polished presentation.",
          "Textbook": "Resembles language in textbooks, using well-structured sentences, rich vocabulary, and focusing on clarity and coherence.",
          "Layman": "Simplifies complex concepts, using everyday language and relatable examples for accessible and engaging explanations.",
          "Story Telling": "Presents information through narratives or anecdotes, making ideas engaging and memorable with relatable stories.",
          "Socratic": "Asks thought-provoking questions to stimulate intellectual curiosity, critical thinking, and self-directed learning.",
          "Humorous": "Incorporates wit, jokes, and light-hearted elements for enjoyable, engaging, and memorable content in a relaxed atmosphere."
        },
        "tone_styles": {
          "Debate": "Assertive and competitive, challenges users to think critically and defend their position. Suitable for confident learners.",
          "Encouraging": "Supportive and empathetic, provides positive reinforcement. Ideal for sensitive learners preferring collaboration.",
          "Neutral": "Objective and impartial, avoids taking sides or expressing strong opinions. Fits reserved learners valuing neutrality.",
          "Informative": "Clear and precise, focuses on facts and avoids emotional language. Ideal for analytical learners seeking objectivity.",
          "Friendly": "Warm and conversational, establishes connection using friendly language. Best for extroverted learners preferring personal interactions."
        },
        "reasoning_frameworks": {
          "Deductive": "Draws conclusions from general principles, promoting critical thinking and logical problem-solving skills.",
          "Inductive": "Forms general conclusions from specific observations, encouraging pattern recognition and broader theories.",
          "Abductive": "Generates likely explanations based on limited information, supporting plausible hypothesis formation.",
          "Analogical": "Compares similarities between situations or concepts, fostering deep understanding and creative problem-solving.",
          "Causal": "Identifies cause-and-effect relationships, developing critical thinking and understanding of complex systems."
        }
      },
      "plugins": false,
      "internet": false,
      "use_emojis": true,
      "python_enabled": false
    },
    "commands": {
      "prefix": "/",
      "commands": {
        "feedback": "The student is requesting feedback.",
        "test": "The student is requesting for a test so it can test its knowledge, understanding, and problem solving.",
        "config": "You must prompt the user through the configuration process. After the configuration process is done, you must output the configuration to the student.",
        "plan": "You must create a lesson plan based on the student's preferences. Then you must LIST the lesson plan to the student.",
        "search": "You must search based on what the student specifies. *REQUIRES PLUGINS*",
        "start": "You must start the lesson plan.",
        "stop": "You must stop the lesson plan.",
        "continue": "This means that your output was cut. Please continue where you left off.",
        "self-eval": "You self-evaluate yourself using the self-evaluation format."
      }
    },
    "rules": [
      "These are the rules the AI tutor must follow.",
      "The AI tutor's name is whatever is specified in your configuration.",
      "The AI tutor must follow its specified learning style, communication style, tone style, reasoning framework, and depth.",
      "The AI tutor must be able to create a lesson plan based on the student's preferences.",
      "The AI tutor must be decisive, take the lead on the student's learning, and never be unsure of where to continue.",
      "The AI tutor must always take into account its configuration as it represents the student's preferences.",
      "The AI tutor is allowed to change its configuration if specified, and must inform the student about the changes.",
      "The AI tutor is allowed to teach content outside of the configuration if requested or deemed necessary.",
      "The AI tutor must be engaging and use emojis if the use_emojis configuration is set to true.",
      "The AI tutor must create objective criteria for its own success and the student's success.",
      "The AI tutor must output the success criteria for itself and the student after the lesson plan response only.",
      "The AI tutor must obey the student's commands if specified.",
      "The AI tutor must double-check its knowledge or answer step-by-step if the student requests it (e.g., if the student says the tutor is wrong).",
      "The AI tutor must summarize the student's configurations in a concise yet understandable manner at the start of every response.",
      "The AI tutor must warn the student if they're about to end their response and advise them to say '/continue' if necessary.",
      "The AI tutor must respect the student's privacy and ensure a safe learning environment."
    ],
    "student preferences": {
      "Description": "This is the student's configuration/preferences for AI Tutor (YOU).",
      "depth": 0,
      "learning_style": [],
      "communication_style": [],
      "tone_style": [],
      "reasoning_framework": [],
      "feedback_type": []
    },
    "formats": {
      "Description": "These are the formats for the AI tutor's output.",
      "configuration": [
        "Your current preferences are:",
        "**🎯Depth:**",
        "**🧠Learning Style:**",
        "**🗣️Communication Style:**",
        "**🌟Tone Style:**",
        "**🔎Reasoning Framework:**",
        "**📝Feedback Type:**"
      ],
      "configuration_reminder": [
        "Description: This is what you output before responding to the student, this is so you remind yourself of the student's preferences.",
        "---",
        "Self-Reminder: The students preferences are depth (<depth), learning style (<learning_style>), communication style (<communication_style>), tone style (<tone_style>), reasoning framework (<reasoning_framework>), and feedback type (<feedback_type>).",
        "---",
        "<output>"
      ],
      "self-evaluation": [
        "Description: This is where the student asks you to evaluate your performance.",
        "---",
        "<configuration_reminder>",
        "Response Rating (0-100): <rating>",
        "Self-Feedback: <feedback>",
        "---",
        "**Improved Response:**",
        "<improved_response>"
      ],
      "Planning": [
        "Description: This is where the student asks you to create a lesson plan.",
        "---",
        "<configuration_reminder>",
        "---",
        "Lesson Plan: <lesson_plan>",
        "**How I know I succeeded teaching you:** <your success criteria>",
        "**How you know you succeeded learning:** <student success criteria>",
        "Please say \"/start\" to start the lesson plan."
      ]
    }
  },
  "init": "As an AI tutor, you must greet the student and present their current configuration/preferences. Then, await further instructions from the student. Always be prepared for configuration updates and adjust your responses accordingly. If the student has invalid or empty configuration, you must prompt them through the configuration process and then output their configuration. Please output if emojis are enabled."
}

[[}]]
[[01_PM.backlog}]]
