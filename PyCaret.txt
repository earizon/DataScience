# https://pycaret.org/ low-code ML Framework: [[{pycaret.101,low_code,python,101]]
 Summary extracted from (PyCaret Summary) @[https://insaid.medium.com/a-complete-guide-to-pycaret-c07b1e51f698]
 ▸ PyCaret: wrapper around many ML models and frameworks (XGBoost, Scikit-learn, ...)
 ▸ end-to-end ML life-cycle including: [[{mlops}]]
   - data preprocessing.
   - train multiple models SIMULTANEOUSLY and outputs a table
     comparing the performance of each one by considering a few
     performance metrics such as precision, recall, f1-score, ...

  ┌─ PyCaret SETUP: ────────────────────────────────────┐
  │ $ docker run -p 8888:8888 pycaret/slim  <··· Alt. 1 │
  │ $ docker run -p 8888:8888 pycaret/full  <··· Alt. 2 │
  │                                                     │
  │ $ conda create -n yourenvname \         <··· Alt. 3 (Windows with no docker)
  │   python=x.x anaconda                               │
  │ $ conda activate $ENV_NAME              <··· Once finished:
  │                                              $ conda deactivate
  │ $ jupyter notebook                           $ conda remove -n $ENV_NAME --all
  │ # !pip install pycaret                              │
  │ · Collecting pycaret, yellowbrick, scikit-learn, ...│
  │                                                     │
  │ NOTE: Other alternatives includ manual install,.... │
  └─────────────────────────────────────────────────────┘
  ┌─ PyCaret TEST SETUP: ───────────────────────────────┐
  │ import pycaret                                      │
  │ pycaret.__version__  # '2.0.0'                      │
  │ from pycaret.utils import enable_colabenable_colab()<·· When using Google colab ONLY
  │ from pycaret.datasets import get_data               │
  │ get_data('index')                                   <·· Check pre-loaded data
  │ > Dataset   Data-Types    Default   Target   Target   #Instances  #Attri-  Missing
  │ >                          task     Var.1    Var.2                 butes   Values
  │ > --------  ------------  --------- -------- -------- ----------  -------  -------
  │ > anomaly   Multivariate  Anomaly   None     None      1000       10       N
  │ >                         Detection
  │ > bank      Multivariate  Classif.  deposit  None     45211       17       N
  │ >                         (Binary)
  │ > blood     Multivariate  Classif.  Class    None       748       5        N
  │ >                         (Binary)                  │
  │ > ...                                               │
  └─────────────────────────────────────────────────────┘
# PYCARET USSAGE:
  # ────────────────────────────────────────────  STEP 0) IMPORT REQUIRED MODULES
  from pycaret.regression     import *
  from pycaret.classification import *
  from pycaret.clustering     import *
  from pycaret.anomaly        import *
  from pycaret.nlp            import *
  from pycaret.arules         import * # association rule mining
  # ──────────────────────────────────────────── STEP 1) INITIALIZE THE SETUP (MANDATORY)
     regression = setup(
                   data ,            # DataFrame with input data used for training
                   target = 'medv')  # target variable name (in input data)j
  #  └──────────┬────────────┘       # NOTE: all the experiment done is saved in a                     [[{mlops]]
  #             ·                    #       pipeline for production deployments                       [[}]]
  #             └··························· AMONGS OTHERS setup(...) COVERS:                          [[{01_doc_has.KEY_POINT]]
  #                                          │1) DATA TYPE INFERENCE FOR FEATURES.
  #  (Output will be similar to):            │2) DATA CLEANING AND PREPARATION.
  #  > Processing +++·········               │   - autoimputes missing val: numerical   data -> mean.
  #  > Initiated ...  (Status, ETC, ...)     │                              categorical data -> mode.
  #  > "Following data types have been       │   - Encoding of Category features.
  #     inferred automatically..."           │3) auto.splits data into train-and-test for modeling.
  #  >           crim     zn       ... medv  │    For classification problems, it uses stratified splits.
  #  > Data Type Numeric  Numeric  ... Label │    By default, the split ratio is 70% train / 30% test. [[}]]
  #  > ...
  #  > SETUP SUCCESFULLY COMPLETED.
  #  >    Description                Value
  #  > 0  session_id                 3201       <·· pseudo-random number (or provided 'session_id')    [[{qa]]
  #  > 1  Transform Target           False          used as "seed" in next functions to isolate the
  #  > 2  Transform Target Method    None           effect of randomization to allow reproducibility.  [[}]]
  #  > 3  Original Data              (506, 14)
  #  > 4  Missing Values             False      > 7  Ordinal Features           False  > 30 Clustering                 False
  #  > 5  Numeric Features           11         > 8  High Cardinality Features  False  > 31 Clustering Iteration       None
  #  > 6  Categorical Features       2          > 9  High Cardinality method    None   > 32 Polynomial Features        False
  #  > 10 Sampled Data               (506, 14)  > 20 PCA Method                 None   > 33 Polynomial Degree          None
  #  > 11 Transformed Train Set      (354, 22)  > 21 PCA Components             None   > 34 Trigonometry Features      False
  #  > 12 Transformed Test  Set      (152, 22)  > 22 Ignore Low Variance        False  > 35 Polynomial Threshold       None
  #  > 13 Numeric Imputer            mean       > 23 Combine Rare Levels        False  > 36 Group Features             False
  #  > 14 Categorical Imputer        constant   > 24 Rare Level Threshold       None   > 37 Feature Selection          False
  #  > 15 Normalize                  False      > 25 Numeric Binning            False  > 38 Features Selection Thresh. None
  #  > 16 Normalize Method           None       > 26 Remove Outliers            False  > 39 Feature Interaction        False
  #  > 17 Transformation             False      > 27 Outliers Threshold         None   > 40 Feature Ratio              False
  #  > 18 Transformation Method      None       > 28 Remove Multicollinearity   False  > 41 Interaction Threshold      None
  #  > 19 PCA                        False      > 29 Multicolinearity Threshold None
  #
  #  NOTE: Evaluation of every ML model and hyperparameter [[{01_PM.TODO]]
  #  optimization is done using K-Fold Cross-Validation.   [[}]]
  # ──────────────────────────────────────────── STEP 2) CREATE A MODEL {
  create_model( 'dt',fold = n)
  #             ·  · └───────┴─· Optional (def: 10) . used for cross-validation during training.
  #             └──┴············ on of 'lr', 'knn', ... See table 1 for a list of existing model_IDs.
  #                              output will be a table of all the metrics rounded up to 4 decimal
  #                              digits as an output.
  #                              For Classification ->: Accuracy, AUC, Recall, Precision, F1, Kappa, MCC
  #                              For     Regression ->: MAE, MSE, RMSE, R2, RMSLE, MAPE
  # ──────────────────────────────────────────── STEP 3) COMPARE MODELS } {
  compare_models()
  # OUTPUT: highest-to-lowest sorted table by the metric of choice
  # - optional  parameters:                        Training of every model is done using the default hyperparameters
  #   - 'fold' (number of folds, def. 10)          evaluates performance metrics using the cross-validation.
  #   - 'sort' param used to sort ouput (def. to 'Accuracy' for classification an R2 for regression)
  #   - 'n_select' (number) : select only top n numbers of the model.

  # WARN: Certain models are prevented for comparison because of their longer run-time.
  #       This cam be prevented  with 'turbo' = False.
  # ──────────────────────────────────────────── STEP 4) TUNE MODELS HYPERPARAMETERS } {
  dt = create_model('dt')               <··· 'dt' stands for the Decision Tree (arbitrary name, Could it be 'dt01?'
  tuned = tune_model(dt, n_iter = 50)   <··· PyCaret used a Random-grid-search with fully customizable
                                             pre-defined grids.
  # ──────────────────────────────────────────── STEP 5) PLOT MODEL } {
  plot_model(model)  # See table 2 for available plots

  interpret_model(model)             <········ [[}]] NEXT ) INTERPRET MODEL RESULTS [[{]]
                                               debug/analyze what the model really thinks is important).


  finalize_model(pre_trained_model)  <········ [[}]] NEXT ) FINALIZE MODEL [[{]]
                                                returns a model trained on the entire dataset.

  save_model(fully_trained_model)    <········ [[}]] NEXT ) ALT.1: SAVES PIPELINE + TRAINED MODEL [[{]]
                                                            (as pickle file)
  deploy_model(final_model,          <········ [[}]] NEXT ) ALT.2: DEPLOY TO CLOUD 'S3' STORAGE  [[{]]
       model_name = 'Model_name_aws',                        NOTE: Default output format must be left blank
       platform = 'aws',                                           in ~/.aws/config
       authentication = {'bucket' :
                         'pycaret-test'}
  )
   predictions = predict_model(      <········ [[}]] NEXT ) USING THE MODEL [[{]]
       model_name = 'lr_aws',
       data = new_or_future_data,
       platform = 'aws',
       authentication = {'bucket' :
                         'pycaret-test'}
  )
                                               [[}]]

  ┌─ TABLE 1: MODEL_ID's used as first parameter in create_model (model_ID, ...) ────────
  │ ▸ CLASSIFICATION MODEL_ID:            · ▸ REGRESSION MODEL_ID :
  │ ‘lr’: Logistic Regression             · ‘lr’ : Linear Regression
  │ ‘knn’: K Nearest Neighbour            · ‘lasso’: Lasso Regression
  │ ‘nb’: Naives Bayes                    · ‘ridge’: Ridge Regression
  │ ‘dt’: Decision Tree Classifier        · ‘en’ : Elastic Net
  │ ‘svm’: SVM – Linear Kernel            · ‘lar’: Least Angle Regression
  │ ‘rbfsvm’: SVM – Radial Kernel         · ‘llar’: Lasso Least Angle Regression
  │ ‘gpc’: Gaussian Process Classifier    · ‘omp’: Orthogonal Matching Pursuit
  │ ‘mlp’: Multi Level Perceptron         · ‘br’ : Bayesian Ridge
  │ ‘ridge’: Ridge Classifier             · ‘ard’: Auto. Relevance Determination
  │ ‘rf’: Random Forest Classifier        · ‘par’: Passive Aggressive Regressor
  │ ‘qda’: Quadratic Discriminant Anal.   · ‘ransac’: Random Sample Consensus
  │ ‘ada’: Ada Boost Classifier           · ‘tr’ : TheilSen Regressor
  │ ‘gbc’: Gradient Boosting Classifi.    · ‘huber’ : Huber Regressor
  │ ‘lda’: Linear Discriminant Analy.     · ‘kr’ : Kernel Ridge
  │ ‘et’: Extra Trees Classifier          · ‘svm’: Support Vector Machine
  │ ‘xgboost’: Extreme Gradient Boost.    · ‘knn’: K Neighbors Regressor
  │ ‘lightgbm’: Light Gradient Boost.     · ‘dt’ : Decision Tree
  │ ‘catboost’: CatBoost Classifier       · ‘rf’ : Random Forest
  │                                       · ‘et’ : Extra Trees Regressor
  │ ▸ NLP MODEL_IDs:                      · ‘ada’: AdaBoost Regressor
  │ ‘lda’: Latent Dirichlet Allocation    · ‘gbr’: Gradient Boosting Regressor
  │ ‘lsi’: Latent Semantic Indexing       · ‘mlp’: Multi Level Perceptron
  │ ‘hdp’: Hierarch.Dirichlet Process     · ‘xgboost’: Extreme Gradient Boosting
  │ ‘rp’ : Random Projections             · ‘lightgbm’: Light Gradient Boosting
  │ ‘nmf’: Non-Negative Matrix Factor.    · ‘catboost’: CatBoost Regressor
  │                                       ·
  │ ▸ CLUSTERING:                         · ▸ ANOMALY DETECTION:
  │ ‘kmeans’: k─means clustering          · ‘abod’   : Angle-base Outlier Detection
  │ ‘ap’    : affinity propagation        · ‘iforest’: Isolation Forest
  │ ‘meanshift’:  mean shift clustering   · ‘cluster’: Clustering-Based Local Outlier
  │ ‘sc’    : spectral clustering         · ‘cof’    : Connectivity-Based Outlier Factor
  │ ‘hclust’: agglomerative clustering    · ‘histogram’ Histogram-based Outlier Detection
  │ ‘dbscan’: density-based spatial clust.· ‘knn’    : k-Nearest Neighbors Detector
  │ ‘optics’: optics clustering           · ‘lof’    : Local Outlier Factor
  │ ‘birch’ : birch clustering            · ‘svm’    : One-class SVM detector
  │ ‘kmodes’: k-modes clustering          · ‘pca’    : Principal Component Analysis
  │                                       · ‘mcd’    : Minimum Covariance Determinant
  │                                       · ‘sod’    : Subspace Outlier Detection
  │                                       · ‘sos‘    : Stochastic Outlier Selection
  └─────────────────────────────────────────────────────────────────────────────────

  ┌─ TABLE 2: Available plots in plot_model(...) ──────────────────────────
  │ ▸ CLASSIFICATION:                     ▸ REGRESSION:
  │   ‘auc’ (Area Under Curve)              ‘residuals’
  │   ‘threshold’ (Discriminat.Thres.)      ‘error’ (Prediction Error)
  │   ‘pr’ (Precision Recall Curve)         ‘cooks’ (Cooks Distance)
  │   ‘confusion_matrix’                    ‘rfe’ (Recursive Feat.Select.)
  │   ‘error’ (Prediction Error)            ‘learning’ (Curve)
  │   ‘class_report’                        ‘vc’ (Validation Curve)
  │   ‘boundary’ (Decision Boundary)        ‘manifold’ (Manifold Learning)
  │   ‘rfe’ ( Recursive Feature Selec.)     ‘feature’ (Feature Importance)
  │   ‘learning’ (Learning Curve)           ‘parameter’ (Model Hyperparam)
  │   ‘manifold’ (Manifold Learning)
  │   ‘calibration’ (Calibration Curve)   ▸ CLUSTERING:
  │   ‘vc’ (Validation Curve)               ‘tsne’ (Cluster TSnE (3d))
  │   ‘dimension’ (Dimension Learning)      ‘elbow’
  │   ‘feature’ (Feature Importance)        ‘silhouette’
  │   ‘parameter’ (Model Hyperparameter)    ‘distance’
  │                                         ‘distribution’
  │
  │ ▸ NLP:                                ▸ ANOMALY DETECTION:
  │   ‘frequency’ (Word Token Freq.)        ‘tsne’ (t-SNE (3d) Dimension)
  │   ‘distribution’ (Word Distr.)          ‘umap’ (UMAP Dimensionality)
  │   ‘bigram’ (Bigram Frequency )
  │   ‘trigram’ (Trigram Frequency )
  │   ‘sentiment’ (Sentiment Polarity )
  │   ‘pos’ (Part of Speech Frequency)
  │   ‘tsne’ (t-SNE (3d) Dimension )
  │   ‘topic_model’ ((pyLDAvis))
  │   ‘topic_distribution’ (Top.Infer.Dist)
  │   ‘wordcloud’ (Word cloud)
  │   ‘umap' (UMAP Dimensionality )
  └────────────────────────────────────────────────────────────────────────










 ▸ PyCaret, See also:
   https://medium.com/@insaid/classification-in-pycaret-bd96bb0f31ae
   https://medium.com/@insaid/regression-in-pycaret-c034bf442416
   https://insaid.medium.com/anomaly-detection-using-pycaret-38b267ed638b
   https://insaid.medium.com/clustering-using-pycaret-964650e32109Let’s Get Started with PyCaret!!!
[[pycaret.101}]]


# ML: automl has a project’s life cycle,
which includes data cleaning, feature selection/engineering, model
selection, parameter optimization, and finally, model validation.
#AutoML came to automate the entire process from data cleaning to
parameter optimization. There are multiple solutions.
In #Python:#PyCaret: https://lnkd.in/dZFfERb#alteryx:
https://lnkd.in/dEk8vKv#autogluon
https://lnkd.in/d3QVtnb#h2o
https://lnkd.in/dFC6JSt#auto-sklearn
https://lnkd.in/dNz_-h7#mlbox
https://lnkd.in/dkEA9-K#tpot
https://lnkd.in/dMxY84b#autokeras
https://lnkd.in/dAwH2jc
In #CloudComputing:#Amazon #AWS SageMaker:
https://lnkd.in/d8wkaGU#Google AutoML
https://lnkd.in/dbCEXY8#Microsoft AutoML
https://lnkd.in/dSCcCNjMore
at: https://lnkd.in/dFe3V7W
More at:
https://lnkd.in/dAXtUtRImage
https://lnkd.in/dicwDCF

# (ML Very low code) Pycaret + Gradio tutorial   [[{01_PM.low_code]]
https://towardsdatascience.com/supercharge-your-machine-learning-experiments-with-pycaret-and-gradio-5932c61f80d9 
  PyCaret is an alternate low-code library that can be used to replace
  hundreds of lines of code with few lines only. This makes the experiment cycle exponentially fast and efficient.PyCaret is simple and easy to use. All the operations performed in PyCaret are
  sequentially stored in a Pipeline that is fully automated for
  deployment. Whether it’s imputing missing values, one-hot-encoding,
  transforming categorical data, feature engineering, or even
  hyperparameter tuning, PyCaret automates all of it.To learn more
  about PyCaret, check out their GitHub. GradioGradio is an open-source
  Python library for creating customizable UI components around your
  machine learning models. Gradio makes it easy for you to “play
  around” with your model in your browser by dragging and dropping in
  your own images, pasting your own text, recording your own voice,
  etc., and seeing what the model outputs.Gradio is useful for:Creating
  quick demos around your trained ML pipelinesGetting live feedback on
  model performanceDebugging your model interactively during
  developmentTo learn more about Gradio, check out their GitHub.
[[}]]

# Anomaly detection with PyCaret
  https://towardsdatascience.com/build-your-first-anomaly-detector-in-power-bi-using-pycaret-2b41b363244e

# ML: ApaflowAPAflow (Advanced and Predictive Analytics Flow) is a
free desktop application for Windows for data science and machine
learning built on top of PyCaret and H2O. APAflow is built for data
analysis, data preparation, feature engineering, model development,
and deployment.Learn more about APAflow:
https://lnkd.in/eVU_nvA

# ML: PyCaret timeseries modulePyCaret Time Series Module
   What you see below is the output from the compare_models of our new
  (in-making) time series module. PyCaret Time Series Module will be an
  easy-to-use forecasting tool built on top of sktime, fbprophet,
  statsmodels, and few others. This module will have both the existing
  functional API as well as a new OOP API that supports object-oriented
  programing. Time Series module has a rich model zoo containing
  classical statistical models such as ARIMA, SARIMA, ETS, TBATS, etc.
  as well as all the models from the regression module. Regressors are
  used with a technique called Reduction Method which converts
  univariate time series into tabular data.This module comes with 50+
  pre-built pipelines with target transformations. The inference will
  be through the predict_model function. A bunch of plotting and
  analysis functionality, auto logging via MLFlow, and deployment
  capabilities.Hyperparameter tuning is auto with the tune_model
  function just like the other modules in PyCaret. This module is a
  result of tireless efforts and hard work by Nikhil Gupta Miguel Trejo
  Marrufo and Antoni Baum as always!#datascience #machinelearning
  #opensource #python #programing #pycaret #timeseries #analytics
  #artificialintelligencve #ai #datascientist

# ML: How to use PyCaret  - the library for easy ML
  https://towardsdatascience.com/how-to-use-pycaret-the-library-for-lazy-data-scientists-91343f960bd2 

# ML ayesian-hyperparameter-optimization-with-tune-sklearn-in-pycaret
  https://medium.com/distributed-computing-with-ray/bayesian-hyperparameter-optimization-with-tune-sklearn-in-pycaret-a33b1592662f

# ML Machine Learning in KNIME with PyCaret | by Moez Ali | Low Code for Advanced Data Science | Medium
  https://medium.com/low-code-for-advanced-data-science/machine-learning-in-knime-with-pycaret-420346e133e2

# https://medium.com/analytics-vidhya/pycaret-101-for-beginners-27d9aefd34c5

# A step-by-step tutorial on unsupervised anomaly detection for time series data using PyCaret
  https://towardsdatascience.com/time-series-anomaly-detection-with-pycaret-706a6e2b2427

# Topic Modeling in Power BI using PyCaret | by Moez Ali | Towards Data Science
https://towardsdatascience.com/topic-modeling-in-power-bi-using-pycaret-54422b4e36d6

# https://www.youtube.com/watch?v=Rgr_3iXORmo
  Anomaly Detection with Power BI and PyCaret

# low_code, pycaret ML Pipeline:
  https://www.datasource.ai/uploads/624e8836466a40923b64b901b5050c0f.html
